{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e46381",
   "metadata": {},
   "source": [
    "# Minibatch Stochastic Gradient Descent\n",
    "At the heart of this is to use minibatches because is computationally efficienct. This is most easily understood when considering parallelization to multiple GPUs and multiple servers.\n",
    "\n",
    "The way to alleviate constraints is to use a hierarchy of CPU caches that are actually fast enough to supply the processor with data. This is the driving force behind batching.\n",
    "1. We could compute $\\mathbf{A}_ij=\\mathbf{B}_{i,:}\\mathbf{C}_{:,j}$ i.e., we could compute it elementwise by means of dot products\n",
    "2. We could compute $\\mathbf{A}_{:,j}=\\mathbf{BC}_{i,:}$ i.e., we could compute it one column at a time. Likewise we could compute $\\mathbf{A}$ one row $\\mathbf{A}_{i,:}$ at a time\n",
    "3. We could simply compute $\\mathbf{A}=\\mathbf{BC}$\n",
    "4. We could break $\\mathbf{B}$ and $\\mathbf{C}$ into smaller block matrices and compute $\\mathbf{A}$ one block at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8bbb6",
   "metadata": {},
   "source": [
    "Option 1 will need to copy one row and one column vector into the CPU each time we want to compute an element $\\mathbf{A}_{ij}$\n",
    "\n",
    "Option 2 is more favorable, we are able to keep the column vector $\\mathbf{C}_{:,j}$ in the CPU cache while we keep on traversing through $\\mathbf{B}$, this halves the memory bandwidth\n",
    "\n",
    "Option 3 is most desirable but most matrices might not entirely fit into cache\n",
    "\n",
    "Option 4 offers practically useful alternative: we can move blocks of the matrix into cache and multiply them locally.\n",
    "\n",
    "Beyond computational efficiency, the overhead introduced by Python and by the learning framework itself is considerable.\n",
    "\n",
    "__Use vectorization (and matrices) whenever possible__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915f0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import torch\n",
    "import numpy as numpy \n",
    "from torch import nn\n",
    "\n",
    "A = torch.zeros(256, 256)\n",
    "B = torch.randn(256, 256)\n",
    "C = torch.randn(256, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4c809",
   "metadata": {},
   "source": [
    "Element-wise assignment simply iterates over all rows and columns of $\\mathbf{B}$ and $\\mathbf{C}$ respectively to assign the value to $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b74fc20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for element-wise operation 1.1361885070800781\n"
     ]
    }
   ],
   "source": [
    "# Compute A = BC one element at a time\n",
    "start = time.time()\n",
    "for i in range(256):\n",
    "    for j in range(256):\n",
    "        A[i,j] = torch.dot(B[i, :], C[:, j])\n",
    "print(\"Time for element-wise operation\",  time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073c3bd",
   "metadata": {},
   "source": [
    "A faster strategy to perform column-wise assignement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5d7c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for column-wise operation 0.026204586029052734\n"
     ]
    }
   ],
   "source": [
    "# Compute A = BC one column at a time\n",
    "start = time.time()\n",
    "\n",
    "for j in range(256):\n",
    "    A[:, j] = torch.mv(B, C[:, j])\n",
    "print(\"Time for column-wise operation\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e1ba2",
   "metadata": {},
   "source": [
    "Lastly the most effective manner is to perform the entire operation in one block. Note that multiplying any two matrices $\\mathbf{B} \\in \\mathbb{R}^{m\\times n}$ and $\\mathbf{C}\\in \\mathbb{R}^{n\\times p}$ kates approximately $2mnp$ floating point operations, when scalar multiplication and addition are counted as separate operations (fused in practice). Thus, multiplying two $256\\times 256$ matrices takes $0.03$ billion floating point operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa42159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for entire operation in one block 0.018102645874023438\n"
     ]
    }
   ],
   "source": [
    "# Compute A = BC in one go\n",
    "start = time.time()\n",
    "A = torch.mm(B, C)\n",
    "print(\"Time for entire operation in one block\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd18cab",
   "metadata": {},
   "source": [
    "### Minibatches\n",
    "We would read _minibatches_ of data rather than single observations to update parameters. Processing single observations requires us to perform many single matrix-vector multiplications, which is quite expensive and which incurs a significant overhead on behalf of the underlying deep learning framework. This applies both to evaluating a network when applied to data and when computing gradients to update parameters\n",
    "\n",
    "Whenever we perform $\\mathbf{w}\\leftarrow\\mathbf{w}-\\eta_t\\mathbf{g}_t$\n",
    "$$\\mathbf{g}_t=\\partial_wf(\\mathbf{x}_t,\\mathbf{w})$$\n",
    "WE can increase the computational efficiency of this operation by applying it to a minibarch of observations at a time. We replace the gradient $\\mathbf{g}_t$ over a single observation by one over a small batch\n",
    "$$\\mathbf{g}_t=\\partial_w\\frac{1}{|\\mathcal{B}_t|}\\sum_{i\\in \\mathcal{B}_t}f(\\mathbf{x}_i,\\mathbf{w})$$\n",
    "\n",
    "Lets see an example of \n",
    "matrix-matrix multiplication but broken up into \"minibatches\" of 64 columns at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e4e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for column-wise operation: 0.001006 seconds\n",
      "Performance in Gigaflops: 29.831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "for j in range(0, 256, 64):\n",
    "    A[:, j:j+64] = torch.mm(B, C[:, j:j+64])\n",
    "    \n",
    "elapsed_time = time.time() - start  \n",
    "print(f\"Time for column-wise operation: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "flops = 0.03 / elapsed_time  # Gigaflops\n",
    "print(f'Performance in Gigaflops: {flops:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vasudeva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
